{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6233846,"sourceType":"datasetVersion","datasetId":3581083}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Import Required Libraries**\n\n* Imports essential deep learning and image processing libraries.\n* Uses TensorFlow/Keras for model building.\n* Uses skimage.metrics for PSNR and SSIM evaluations.\n* Uses cv2 (OpenCV) for UIQM (Underwater Image Quality Measure) calculations.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:39:11.452047Z","iopub.execute_input":"2025-05-18T17:39:11.452746Z","iopub.status.idle":"2025-05-18T17:39:24.124229Z","shell.execute_reply.started":"2025-05-18T17:39:11.452719Z","shell.execute_reply":"2025-05-18T17:39:24.123649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Define Dataset Paths**\n\n* Specifies the input paths for underwater images (trainA) and enhanced images (trainB) from the EUVP dataset.","metadata":{}},{"cell_type":"code","source":"# Define dataset paths\ntrainA_path = \"/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainA/\"\ntrainB_path = \"/kaggle/input/euvp-dataset/EUVP/Paired/underwater_dark/trainB/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:39:24.125630Z","iopub.execute_input":"2025-05-18T17:39:24.126147Z","iopub.status.idle":"2025-05-18T17:39:24.129963Z","shell.execute_reply.started":"2025-05-18T17:39:24.126127Z","shell.execute_reply":"2025-05-18T17:39:24.129268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Define a Function to Load Images**\n* Loads an image, resizes it to (256,256), and normalizes pixel values to [0,1].","metadata":{}},{"cell_type":"code","source":"# Create function to load images\ndef load_images(image_path, target_size=(256, 256)):\n    img = load_img(image_path, target_size=target_size)\n    img = img_to_array(img) / 255.0  # Normalize\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:39:24.130717Z","iopub.execute_input":"2025-05-18T17:39:24.131183Z","iopub.status.idle":"2025-05-18T17:39:24.155762Z","shell.execute_reply.started":"2025-05-18T17:39:24.131159Z","shell.execute_reply":"2025-05-18T17:39:24.155236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load the Dataset**\n* Reads paired images from trainA/ and trainB/ directories.\n* Converts them into numpy arrays for training.\n* Prints dataset shape.","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndef load_dataset():\n    x_train, y_train = [], []\n    image_names = os.listdir(trainA_path)\n\n    for img_name in image_names:\n        imgA = load_images(os.path.join(trainA_path, img_name))  # Low-quality\n        imgB = load_images(os.path.join(trainB_path, img_name))  # Enhanced\n        \n        x_train.append(imgA)\n        y_train.append(imgB)\n\n    return np.array(x_train), np.array(y_train)\n\n# Load the training data\nX_train, Y_train = load_dataset()\n\nprint(f\"Dataset Loaded! Shape: {X_train.shape} -> {Y_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:39:24.156443Z","iopub.execute_input":"2025-05-18T17:39:24.156679Z","iopub.status.idle":"2025-05-18T17:41:45.441938Z","shell.execute_reply.started":"2025-05-18T17:39:24.156655Z","shell.execute_reply":"2025-05-18T17:41:45.441045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CBAM Attention Module**\nThe Convolutional Block Attention Module (CBAM) enhances deep learning models by focusing on important features through Channel Attention and Spatial Attention.\n* Input:\n    * The function takes a feature map input_tensor from a CNN layer.\n    * The shape of the input is (H, W, C) where:\n        * H, W = Height & Width of the feature map.\n        * C = Number of channels.\n* Channel Attention:\nGoal: Identify which channels (features) are most important.\n    * Global Average Pooling (GAP): Computes the average value of each channel â†’ Creates a (C,) vector.\n    * Global Max Pooling (GMP): Computes the max value of each channel â†’ Another (C,) vector.\n    * Fully Connected Layers:\n        * First Dense layer (ReLU activation) reduces dimensionality (C // reduction_ratio).\n        * Second Dense layer (Sigmoid activation) restores original dimension C and creates attention weights.\n    * Summation & Reshaping:\n        * Outputs from both GAP and GMP are summed, reshaped into (1, 1, C), and multiplied with the original input\nEffect: Enhances important feature channels.\n\n* Spatial Attention:\nGoal: Identify which regions in the image are most important.\n    * Global Average & Max Pooling (along channels):\n        * Generates two (H, W, 1) maps (one from average pooling, one from max pooling).\n    * Concatenation: Merges the two maps into a (H, W, 2) tensor.\n    * 7Ã—7 Convolution Layer: Applies a sigmoid activation to get a spatial attention map (H, W, 1).\n    * Multiplication: This attention map is multiplied with the previous channel-refined feature map.\nEffect: Highlights important regions in the image.\n* Output:\n    * The refined feature map is returned, containing enhanced channels and spatial regions.\n    * This can be used in models like U-Net, ResNet, etc., to improve feature extraction.","metadata":{}},{"cell_type":"code","source":"# Define CBAM Attention Module\ndef cbam_block(input_tensor, reduction_ratio=16):\n    \"\"\"Convolutional Block Attention Module (CBAM)\"\"\"\n    channels = input_tensor.shape[-1]\n\n    # Channel Attention\n    avg_pool = GlobalAveragePooling2D()(input_tensor)\n    max_pool = GlobalMaxPooling2D()(input_tensor)\n    dense1 = Dense(channels // reduction_ratio, activation=\"relu\")\n    dense2 = Dense(channels, activation=\"sigmoid\")\n    \n    scale = Add()([dense2(dense1(avg_pool)), dense2(dense1(max_pool))])\n    scale = Reshape((1, 1, channels))(scale)\n    channel_attention = Multiply()([input_tensor, scale])\n\n    # Spatial Attention\n    avg_pool_spatial = Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(channel_attention)\n    max_pool_spatial = Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(channel_attention)\n    concat = Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])\n    spatial_attention = Conv2D(1, kernel_size=7, activation=\"sigmoid\", padding=\"same\")(concat)\n\n    return Multiply()([channel_attention, spatial_attention])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:41:45.444198Z","iopub.execute_input":"2025-05-18T17:41:45.444429Z","iopub.status.idle":"2025-05-18T17:41:45.451181Z","shell.execute_reply.started":"2025-05-18T17:41:45.444412Z","shell.execute_reply":"2025-05-18T17:41:45.450421Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CBAM U-Net with VGG16 Encoder**\n* Define Model Function\n    * The function build_model() initializes a U-Net model for underwater image enhancement with input size (256, 256, 3).\n* Encoder (Feature Extraction using VGG16)\n    * Loads VGG16 (pretrained on ImageNet) as the encoder without fully connected layers.\n    * Extracts deep feature maps from specific convolutional layers:\n        * block1_conv2, block2_conv2, block3_conv3, block4_conv3.\n    * Takes the output of block5_conv3 and applies CBAM (Convolutional Block Attention Module) to enhance feature importance.\n* Decoder (Upsampling & Skip Connections)\n    * Uses transpose convolution layers to upsample the feature maps step by step.\n    * Skip connections from the encoder layers are added at each step to preserve fine details.\n* Output Layer\n    * Uses a 1Ã—1 convolution with a sigmoid activation to generate the final enhanced RGB image.\n* Model Compilation\n    * Creates a Model(inputs, outputs), compiles it with:\n        * Loss function: MSE (Mean Squared Error)\n        * Optimizer: Adam (1e-4 learning rate)\n        * Metrics: MAE (Mean Absolute Error)\nâœ… Result: A CBAM U-Net model that learns to enhance underwater images by preserving details and improving color contrast! ðŸŒŠðŸ”¥","metadata":{}},{"cell_type":"code","source":"# Define CBAM U-Net with VGG16 Encoder\ndef build_model(input_shape=(256, 256, 3)):\n    inputs = Input(shape=input_shape)\n    \n    # Encoder (Pretrained VGG16)\n    base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n    \n    skips = [base_model.get_layer(name).output for name in [\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"]]\n    \n    encoder_output = base_model.get_layer(\"block5_conv3\").output\n    encoder_output = cbam_block(encoder_output)\n\n    # Decoder\n    x = Conv2DTranspose(512, (3,3), strides=(2,2), padding=\"same\")(encoder_output)\n    x = Concatenate()([x, skips[-1]])\n    \n    x = Conv2DTranspose(256, (3,3), strides=(2,2), padding=\"same\")(x)\n    x = Concatenate()([x, skips[-2]])\n\n    x = Conv2DTranspose(128, (3,3), strides=(2,2), padding=\"same\")(x)\n    x = Concatenate()([x, skips[-3]])\n\n    x = Conv2DTranspose(64, (3,3), strides=(2,2), padding=\"same\")(x)\n    x = Concatenate()([x, skips[-4]])\n\n    x = Conv2D(3, (1,1), activation=\"sigmoid\")(x)\n\n    model = Model(inputs, x)\n    model.compile(optimizer=Adam(1e-4), loss=\"mse\", metrics=[\"mae\"])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:41:45.451955Z","iopub.execute_input":"2025-05-18T17:41:45.452512Z","iopub.status.idle":"2025-05-18T17:41:45.471451Z","shell.execute_reply.started":"2025-05-18T17:41:45.452487Z","shell.execute_reply":"2025-05-18T17:41:45.470631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model Creation & Summary**\n1. Create the Model\n    * Calls the build_model() function, which:\n    * Uses VGG16 as the encoder (pretrained on ImageNet).\n    * Adds a CBAM attention module for feature enhancement.\n    * Builds a decoder with Conv2DTranspose layers for image reconstruction.\n    * Compiles the model with Adam optimizer and MSE loss.\n 2.  Print Model Summary\n  * Displays:\n    * Layer types (Conv2D, BatchNorm, Attention, etc.).\n    * Number of parameters in each layer.\n    * Input & output shape at each stage.","metadata":{}},{"cell_type":"code","source":"# Create model\nmodel = build_model()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:41:45.472316Z","iopub.execute_input":"2025-05-18T17:41:45.472623Z","iopub.status.idle":"2025-05-18T17:41:50.716553Z","shell.execute_reply.started":"2025-05-18T17:41:45.472597Z","shell.execute_reply":"2025-05-18T17:41:50.715981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train the Model**\n* Trains the model using paired images from trainA â†’ trainB.","metadata":{}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X_train, Y_train, batch_size=8, epochs=100, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:41:50.717287Z","iopub.execute_input":"2025-05-18T17:41:50.717583Z","iopub.status.idle":"2025-05-18T20:33:39.015434Z","shell.execute_reply.started":"2025-05-18T17:41:50.717566Z","shell.execute_reply":"2025-05-18T20:33:39.014810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Visualization & Saving Enhanced Images**\n\nThis function visualizes and saves the input, ground truth, and enhanced images.\n* Step-by-Step Breakdown:\n    * Generate Model Predictions\n        * The function takes a trained model, test images (test_images), and ground truth images (test_labels).\n        * It predicts the enhanced image for each input using model.predict().\n        *  np.clip(pred, 0, 1) ensures pixel values are between 0 and 1.\n    * Display Images Using Matplotlib\n\n        * Creates a 3-column figure (fig, ax = plt.subplots(1, 3)).\n        * Displays:\n           *  Input Image\n           *  Ground Truth Image\n           * Enhanced Image (Model Output)\n           * Titles and axes are formatted properly.\n\n    * Convert Images to uint8 Format\n          * Images are scaled from [0,1] to [0,255] and converted to uint8 for saving.\n    * Save Images in Organized Folders\n        *  Saves images in \"results/input_images\", \"results/ground_truth\", and \"results/enhanced_images\".\n        *  cv2.imwrite() is used to save images in BGR format.\n    * Call the Function\n        * visualize_results(model, X_train[:10], Y_train[:10]) â†’ Runs visualization for first 10 samples.","metadata":{}},{"cell_type":"code","source":"# Visualization & Saving Enhanced Images with Subfolders\ndef visualize_results(model, test_images, test_labels, num_samples=20):\n    for i in range(num_samples):\n        pred = model.predict(np.expand_dims(test_images[i], axis=0))[0]\n        pred = np.clip(pred, 0, 1)\n\n        fig, ax = plt.subplots(1, 3, figsize=(12, 5))\n        \n        ax[0].imshow(test_images[i])\n        ax[0].set_title(f\"Input Image {i+1}\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(test_labels[i])\n        ax[1].set_title(f\"Ground Truth {i+1}\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(pred)\n        ax[2].set_title(f\"Enhanced Image {i+1}\")\n        ax[2].axis(\"off\")\n\n        plt.show()\n\n        # Convert images to uint8 format for saving\n        input_img = (test_images[i] * 255).astype(np.uint8)\n        gt_img = (test_labels[i] * 255).astype(np.uint8)\n        pred_img = (pred * 255).astype(np.uint8)\n\n        # Save images in respective folders\n        cv2.imwrite(f\"results/input_images/input_{i+1}.png\", cv2.cvtColor(input_img, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(f\"results/ground_truth/gt_{i+1}.png\", cv2.cvtColor(gt_img, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(f\"results/enhanced_images/enhanced_{i+1}.png\", cv2.cvtColor(pred_img, cv2.COLOR_RGB2BGR))\n\n# Visualize & Save\nvisualize_results(model, X_train[:20], Y_train[:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:39:54.791425Z","iopub.execute_input":"2025-05-18T20:39:54.791698Z","iopub.status.idle":"2025-05-18T20:40:03.160557Z","shell.execute_reply.started":"2025-05-18T20:39:54.791678Z","shell.execute_reply":"2025-05-18T20:40:03.159984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Creating Directories for Storing Results**\n* Check & Create Main \"results\" Folder\n* If the \"results\" folder doesnâ€™t exist, it is created using os.makedirs(\"results\").\n* Create Subfolders for Organizing Output Images\n   \"results/input_images\" â†’ Stores original underwater images.\n   \"results/ground_truth\" â†’ Stores ground truth (reference) images.\n  \"results/enhanced_images\" â†’ Stores model-enhanced images.\n* exist_ok=True Ensures No Errors\n  \nIf the folder already exists, it wonâ€™t throw an error, making the code more robust","metadata":{}},{"cell_type":"code","source":"# Create results folder\nif not os.path.exists(\"results\"):\n    os.makedirs(\"results\")\n\n# Create necessary folders\nos.makedirs(\"results/input_images\", exist_ok=True)\nos.makedirs(\"results/ground_truth\", exist_ok=True)\nos.makedirs(\"results/enhanced_images\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:39:31.358437Z","iopub.execute_input":"2025-05-18T20:39:31.359056Z","iopub.status.idle":"2025-05-18T20:39:31.363068Z","shell.execute_reply.started":"2025-05-18T20:39:31.359033Z","shell.execute_reply":"2025-05-18T20:39:31.362399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ðŸ’¾Saving the Trained Model**\n* What it does:\n    * Saves the trained deep learning model in HDF5 (.h5) format.\n        * The saved model includes:\n        * Model architecture (layers & structure).\n        * Trained weights (learned parameters).\n        * Optimizer state (for resuming training).\n* Why save it?\n        * Allows reloading the model later without retraining.\n        * Can be used for inference or further fine-tuning.\nâœ… The model will be stored at /kaggle/working/enhancement_model.h5, making it accessible for future use. ðŸš€","metadata":{}},{"cell_type":"code","source":"model.save('/kaggle/working/enhancement_model.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:39:47.848770Z","iopub.execute_input":"2025-05-18T20:39:47.849045Z","iopub.status.idle":"2025-05-18T20:39:48.535905Z","shell.execute_reply.started":"2025-05-18T20:39:47.849026Z","shell.execute_reply":"2025-05-18T20:39:48.535132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Imports PSNR and SSIM metrics from skimage and numpy for computations.\n\n* Defines a function evaluate_model to assess model predictions against ground truth images.\n\n* Loops through all test images:\n\n    * Predicts enhanced image using the model.\n\n    * Clips predicted pixel values to the valid range [0, 1].\n\n    * Gets corresponding ground truth image.\n\n* Calculates three metrics for each image pair:\n\n    * PSNR (Peak Signal-to-Noise Ratio)\n\n    * SSIM (Structural Similarity Index)\n\n    * MSE (Mean Squared Error)\n\n* Stores metric values in lists for all images.\n\n* Computes the average PSNR, SSIM, and MSE across all images.\n\n* Prints and returns the average metric values.\n\n* Calls the evaluation function on the first 50 samples from training data","metadata":{}},{"cell_type":"code","source":"from skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport numpy as np\n\n# Evaluate model performance (PSNR, SSIM, MSE)\ndef evaluate_model(model, test_images, test_labels):\n    psnr_vals, ssim_vals, mse_vals = [], [], []\n\n    for i in range(len(test_images)):\n        pred = model.predict(np.expand_dims(test_images[i], axis=0))[0]\n        pred = np.clip(pred, 0, 1)  # Ensure valid pixel range\n\n        gt = test_labels[i]\n\n        # Compute PSNR, SSIM, MSE\n        psnr_vals.append(psnr(gt, pred))\n        ssim_vals.append(ssim(gt, pred, channel_axis=-1, data_range=1.0))\n        mse_vals.append(np.mean((gt - pred) ** 2))  # MSE Calculation\n\n    print(f\"PSNR: {np.mean(psnr_vals):.2f}, SSIM: {np.mean(ssim_vals):.4f}, MSE: {np.mean(mse_vals):.6f}\")\n    return np.mean(psnr_vals), np.mean(ssim_vals), np.mean(mse_vals)\n\n# Evaluate on 50 samples\npsnr_val, ssim_val, mse_val = evaluate_model(model, X_train[:50], Y_train[:50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:33:49.494665Z","iopub.execute_input":"2025-05-18T20:33:49.494884Z","iopub.status.idle":"2025-05-18T20:33:54.421350Z","shell.execute_reply.started":"2025-05-18T20:33:49.494868Z","shell.execute_reply":"2025-05-18T20:33:54.420751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* compute_uiqm(image):\n\n    * Converts input image from float [0,1] to 8-bit uint8.\n\n    * Converts image to LAB color space.\n\n    * Calculates underwater image quality measure (UIQM) based on luminance contrast (std/mean of L) and average color components (mean of        A and B).\n\n    * Returns the weighted UIQM score.\n\n* compute_uciqe(image):\n\n    * Converts input image to 8-bit uint8.\n\n    * Converts image to LAB color space.\n\n    * Calculates chroma from A and B channels.\n\n    * Computes standard deviation of chroma (sigma_c), standard deviation of luminance (con_l), and mean saturation-like value (mean_s).\n\n    * Combines these stats with weighted coefficients to calculate underwater color image quality evaluation (UCIQE).\n\n    * Returns the UCIQE score.\n\n* evaluate_model_uiqm_uciqe(model, input_images):\n\n    * For each input image:\n\n        * Runs the model to get the enhanced prediction.\n\n        * Clips pixel values to valid range.\n\n        * Calculates UIQM and UCIQE for the predicted image.\n\n    * Prints and returns the average UIQM and UCIQE scores across all images.\n\n* Example usage:\n\n    * Calls evaluate_model_uiqm_uciqe on the first 200 images of X_train to get average underwater image quality scores for the enhanced          outputs from the model.","metadata":{}},{"cell_type":"code","source":"def compute_uiqm(image):\n    \"\"\"Calculates UIQM for a given image.\"\"\"\n    image = (image * 255).astype(np.uint8)\n\n    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    L, A, B = cv2.split(lab)\n\n    uc = np.std(L) / np.mean(L)\n    us = np.mean(A) + np.mean(B)\n    uiqm = 0.0282 * uc + 0.2953 * us\n\n    return uiqm\n\n\ndef compute_uciqe(image):\n    \"\"\"Calculates UCIQE (Underwater Color Image Quality Evaluation) for a given image.\"\"\"\n    image = (image * 255).astype(np.uint8)  # Convert to 8-bit\n\n    # Convert to LAB color space\n    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    L, A, B = cv2.split(lab)\n\n    # Calculate chroma\n    chroma = np.sqrt(A.astype(float)**2 + B.astype(float)**2)\n\n    # Calculate statistics\n    sigma_c = np.std(chroma)\n    con_l = np.std(L)\n    mean_s = np.mean(chroma / (L + 1e-6))  # saturation-like measure to avoid div by zero\n\n    # Weighted sum based on original paper\n    uciqe = 0.4680 * sigma_c + 0.2745 * con_l + 0.2576 * mean_s\n    return uciqe\n\n\ndef evaluate_model_uiqm_uciqe(model, input_images):\n    uiqm_vals, uciqe_vals = [], []\n\n    for i in range(len(input_images)):\n        pred = model.predict(np.expand_dims(input_images[i], axis=0))[0]\n        pred = np.clip(pred, 0, 1)\n\n        uiqm_vals.append(compute_uiqm(pred))\n        uciqe_vals.append(compute_uciqe(pred))\n\n    print(f\"UIQM: {np.mean(uiqm_vals):.3f}, UCIQE: {np.mean(uciqe_vals):.3f}\")\n    return np.mean(uiqm_vals), np.mean(uciqe_vals)\n\n# Example call on your dataset\nuiqm_val, uciqe_val = evaluate_model_uiqm_uciqe(model, X_train[:200])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:36:28.294475Z","iopub.execute_input":"2025-05-18T20:36:28.294741Z","iopub.status.idle":"2025-05-18T20:36:45.760994Z","shell.execute_reply.started":"2025-05-18T20:36:28.294722Z","shell.execute_reply":"2025-05-18T20:36:45.760408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Histogram**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.feature import local_binary_pattern\nimport os\n\n# Create directory to save histograms\nsave_dir = 'histograms'\nos.makedirs(save_dir, exist_ok=True)\n\n# Helper function to display and save a histogram plot\ndef plot_histogram(data1, data2, title, color1='blue', color2='red', save_name=None):\n    plt.figure(figsize=(6, 4))\n    plt.hist(data1.ravel(), bins=256, color=color1, alpha=0.5, label='Original')\n    plt.hist(data2.ravel(), bins=256, color=color2, alpha=0.5, label='Enhanced')\n    plt.title(title)\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.grid(True)\n    if save_name:\n        plt.savefig(os.path.join(save_dir, save_name))\n    plt.show()\n    plt.close()\n\n# Load images\norig_img = cv2.imread('/kaggle/working/results/input_images/input_1.png')\norig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\nenh_img = cv2.imread('/kaggle/working/results/enhanced_images/enhanced_1.png')\nenh_img = cv2.cvtColor(enh_img, cv2.COLOR_BGR2RGB)\n\n# 1. Grayscale Histogram\nplot_histogram(cv2.cvtColor(orig_img, cv2.COLOR_RGB2GRAY),\n               cv2.cvtColor(enh_img, cv2.COLOR_RGB2GRAY),\n               '1_Grayscale_Histogram', 'blue', 'red', '1_Grayscale_Histogram.png')\n\n# 2. RGB Histograms per channel\nfor i, color in enumerate(('r', 'g', 'b')):\n    plot_histogram(orig_img[:, :, i], enh_img[:, :, i],\n                   f'2_{color.upper()}_Channel_Histogram',\n                   color1=color, color2='black',\n                   save_name=f'2_{color.upper()}_Channel_Histogram.png')\n\n# 3. Combined RGB Histogram\nplt.figure(figsize=(6, 4))\nfor i, color in enumerate(('r', 'g', 'b')):\n    plt.hist(orig_img[:, :, i].ravel(), bins=256, color=color, alpha=0.4, label=f'Orig-{color.upper()}')\n    plt.hist(enh_img[:, :, i].ravel(), bins=256, color=color, alpha=0.9, histtype='step', label=f'Enh-{color.upper()}')\nplt.title('3_Combined_RGB_Histogram')\nplt.xlabel('Pixel Intensity')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid(True)\nplt.savefig(os.path.join(save_dir, '3_Combined_RGB_Histogram.png'))\nplt.show()\nplt.close()\n\n# 4. HSV Histograms\norig_hsv = cv2.cvtColor(orig_img, cv2.COLOR_RGB2HSV)\nenh_hsv = cv2.cvtColor(enh_img, cv2.COLOR_RGB2HSV)\nfor i, name in enumerate(('Hue', 'Saturation', 'Value')):\n    plot_histogram(orig_hsv[:, :, i], enh_hsv[:, :, i],\n                   f'4_{name}_Channel_Histogram',\n                   save_name=f'4_{name}_Channel_Histogram.png')\n\n# 5. LAB Histograms\norig_lab = cv2.cvtColor(orig_img, cv2.COLOR_RGB2LAB)\nenh_lab = cv2.cvtColor(enh_img, cv2.COLOR_RGB2LAB)\nfor i, name in enumerate(('L', 'A', 'B')):\n    plot_histogram(orig_lab[:, :, i], enh_lab[:, :, i],\n                   f'5_LAB_{name}_Channel_Histogram',\n                   save_name=f'5_LAB_{name}_Channel_Histogram.png')\n\n# 6. YCrCb Histograms\norig_ycrcb = cv2.cvtColor(orig_img, cv2.COLOR_RGB2YCrCb)\nenh_ycrcb = cv2.cvtColor(enh_img, cv2.COLOR_RGB2YCrCb)\nfor i, name in enumerate(('Y', 'Cr', 'Cb')):\n    plot_histogram(orig_ycrcb[:, :, i], enh_ycrcb[:, :, i],\n                   f'6_YCrCb_{name}_Channel_Histogram',\n                   save_name=f'6_YCrCb_{name}_Channel_Histogram.png')\n\n# 7. Global Histogram (same as grayscale)\nplot_histogram(cv2.cvtColor(orig_img, cv2.COLOR_RGB2GRAY),\n               cv2.cvtColor(enh_img, cv2.COLOR_RGB2GRAY),\n               '7_Global_Histogram', 'blue', 'red', '7_Global_Histogram.png')\n\n# 8. Local Histogram (Top-left region)\nor_region = orig_img[:orig_img.shape[0]//2, :orig_img.shape[1]//2]\nen_region = enh_img[:enh_img.shape[0]//2, :enh_img.shape[1]//2]\nplot_histogram(cv2.cvtColor(or_region, cv2.COLOR_RGB2GRAY),\n               cv2.cvtColor(en_region, cv2.COLOR_RGB2GRAY),\n               '8_Local_Histogram_TopLeft', 'blue', 'red', '8_Local_Histogram_TopLeft.png')\n\n# 9. Sliding Window Histogram (100:156,100:156)\nor_win = orig_img[100:156, 100:156]\nen_win = enh_img[100:156, 100:156]\nplot_histogram(cv2.cvtColor(or_win, cv2.COLOR_RGB2GRAY),\n               cv2.cvtColor(en_win, cv2.COLOR_RGB2GRAY),\n               '9_Sliding_Window_Histogram', 'blue', 'red', '9_Sliding_Window_Histogram.png')\n\n# 10. Equalized Histogram\neq1 = cv2.equalizeHist(cv2.cvtColor(orig_img, cv2.COLOR_RGB2GRAY))\neq2 = cv2.equalizeHist(cv2.cvtColor(enh_img, cv2.COLOR_RGB2GRAY))\nplot_histogram(eq1, eq2, '10_Equalized_Histogram', 'blue', 'red', '10_Equalized_Histogram.png')\n\n# 11. Cumulative Histogram\norig_gray = cv2.cvtColor(orig_img, cv2.COLOR_RGB2GRAY)\nenh_gray = cv2.cvtColor(enh_img, cv2.COLOR_RGB2GRAY)\nplt.figure(figsize=(6, 4))\nplt.plot(cv2.calcHist([orig_gray], [0], None, [256], [0,256]).cumsum(), color='blue', label='Original')\nplt.plot(cv2.calcHist([enh_gray], [0], None, [256], [0,256]).cumsum(), color='red', label='Enhanced')\nplt.title('11_Cumulative_Histogram')\nplt.xlabel('Pixel Intensity')\nplt.ylabel('Cumulative Frequency')\nplt.legend()\nplt.grid(True)\nplt.savefig(os.path.join(save_dir, '11_Cumulative_Histogram.png'))\nplt.show()\nplt.close()\n\n# 12. Difference Histogram\ndiff_orig = cv2.absdiff(orig_img, cv2.GaussianBlur(orig_img, (5, 5), 0))\ndiff_enh = cv2.absdiff(enh_img, cv2.GaussianBlur(enh_img, (5, 5), 0))\nplot_histogram(cv2.cvtColor(diff_orig, cv2.COLOR_RGB2GRAY),\n               cv2.cvtColor(diff_enh, cv2.COLOR_RGB2GRAY),\n               '12_Difference_Histogram', 'blue', 'red', '12_Difference_Histogram.png')\n\n# 13. 2D Histogram (R vs G)\nplt.figure(figsize=(6, 4))\nplt.hist2d(orig_img[:, :, 0].ravel(), orig_img[:, :, 1].ravel(), bins=32, cmap='Blues', alpha=0.5)\nplt.hist2d(enh_img[:, :, 0].ravel(), enh_img[:, :, 1].ravel(), bins=32, cmap='Reds', alpha=0.5)\nplt.title('13_2D_Histogram_R_vs_G')\nplt.xlabel('Red')\nplt.ylabel('Green')\nplt.colorbar()\nplt.savefig(os.path.join(save_dir, '13_2D_Histogram_R_vs_G.png'))\nplt.show()\nplt.close()\n\n# 14. 3D Color Histogram (just print shapes)\nhist_orig = cv2.calcHist([orig_img], [0, 1, 2], None, [8, 8, 8], [0,256, 0,256, 0,256])\nhist_enh = cv2.calcHist([enh_img], [0, 1, 2], None, [8, 8, 8], [0,256, 0,256, 0,256])\nprint(\"14. 3D Color Histogram Shapes:\", hist_orig.shape, hist_enh.shape)\n\n# 15. LBP Histogram\nlbp_orig = local_binary_pattern(orig_gray, P=8, R=1, method='uniform')\nlbp_enh = local_binary_pattern(enh_gray, P=8, R=1, method='uniform')\nplot_histogram(lbp_orig, lbp_enh, '15_LBP_Texture_Histogram', 'blue', 'red', '15_LBP_Texture_Histogram.png')\n\n# 16. Gradient Histogram\nsobelx1 = cv2.Sobel(orig_gray, cv2.CV_64F, 1, 0, ksize=5)\nsobely1 = cv2.Sobel(orig_gray, cv2.CV_64F, 0, 1, ksize=5)\nmag1 = np.sqrt(sobelx1**2 + sobely1**2)\n\nsobelx2 = cv2.Sobel(enh_gray, cv2.CV_64F, 1, 0, ksize=5)\nsobely2 = cv2.Sobel(enh_gray, cv2.CV_64F, 0, 1, ksize=5)\nmag2 = np.sqrt(sobelx2**2 + sobely2**2)\n\nplot_histogram(mag1, mag2, '16_Gradient_Magnitude_Histogram', 'blue', 'red', '16_Gradient_Magnitude_Histogram.png')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:50:23.155984Z","iopub.execute_input":"2025-05-18T20:50:23.156790Z","iopub.status.idle":"2025-05-18T20:50:46.655990Z","shell.execute_reply.started":"2025-05-18T20:50:23.156753Z","shell.execute_reply":"2025-05-18T20:50:46.655277Z"}},"outputs":[],"execution_count":null}]}